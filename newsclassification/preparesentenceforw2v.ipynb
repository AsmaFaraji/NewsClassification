{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from __future__ import unicode_literals\n",
    "from hazm import Normalizer, word_tokenize, sent_tokenize,Stemmer,Lemmatizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address = ['/Users/asma/University/DataMining/ThirdUpdateNewsDataSet/Accident/', \n",
    "          '/Users/asma/University/DataMining/ThirdUpdateNewsDataSet/Cultural/',\n",
    "          '/Users/asma/University/DataMining/ThirdUpdateNewsDataSet/Economic/',\n",
    "          '/Users/asma/University/DataMining/ThirdUpdateNewsDataSet/Political/',\n",
    "          '/Users/asma/University/DataMining/ThirdUpdateNewsDataSet/Research/',\n",
    "          '/Users/asma/University/DataMining/ThirdUpdateNewsDataSet/Science/',\n",
    "          '/Users/asma/University/DataMining/ThirdUpdateNewsDataSet/Social/',\n",
    "          '/Users/asma/University/DataMining/ThirdUpdateNewsDataSet/Sport/',\n",
    "          '/Users/asma/University/DataMining/ThirdUpdateNewsDataSet/State/',\n",
    "          '/Users/asma/University/DataMining/ThirdUpdateNewsDataSet/World/']\n",
    "damnword = ['این',\n",
    "            'خود'\n",
    "            , 'بود'\n",
    "            , 'دیگر'\n",
    "            ,'نیز'\n",
    "            ,'اند'\n",
    "            ,'ان'\n",
    "            ,'سال'\n",
    "            ,'گفت'\n",
    "            ,'علیه'\n",
    "            ,'روز'\n",
    "            ,'زیاد'\n",
    "            ,'هنوز'\n",
    "            ,'زاده'\n",
    "            ,'اما'\n",
    "            ,'چنین'\n",
    "            ,'پیش'\n",
    "            ,'کامل'\n",
    "            ,'ابتدا'\n",
    "            ,'ولی'\n",
    "            ,'داشت'\n",
    "            ,'افزود'\n",
    "            ,'بزرگ'\n",
    "            ,'همه'\n",
    "            ,'آقا'\n",
    "            ,'محمد'\n",
    "            ,'علی'\n",
    "            ,'میثم'\n",
    "            ,'علیه'\n",
    "            ,'حال'\n",
    "            ,'انجام'\n",
    "            ,'وجود'\n",
    "            ,'حتی'\n",
    "            ,'باره'\n",
    "            ,'شنبه'\n",
    "            ,'برجا'\n",
    "            ,'داشت'\n",
    "            ,'ممکن'\n",
    "            ,'بعضی'\n",
    "            ,'نیست'\n",
    "            ,'هستند'\n",
    "            ,'منتها'\n",
    "            ,'هیچ'\n",
    "            ,'میبیند'\n",
    "            ,'کند'\n",
    "            ,'حال'\n",
    "            ,'غیر'\n",
    "            ,'برخی'\n",
    "            ,'را'\n",
    "            ,'مگر'\n",
    "            ,'والا'\n",
    "            ,'بسیار'\n",
    "            ,'همانند'\n",
    "            ,'برد'\n",
    "            ,'گذراند'\n",
    "            ,'غیر'\n",
    "            ,'چند'\n",
    "            ,'همان'\n",
    "            ,'حتما'\n",
    "            ,'کلیه'\n",
    "            ,'لذا','کس'\n",
    "            ,'خبرنگار'\n",
    "            ,'برای'\n",
    "            ,'باش'\n",
    "            ,'همین'\n",
    "            ,'شاید'\n",
    "            ,'مند'\n",
    "            ,'فرو'\n",
    "            ,'اگر'\n",
    "            ,'ادامه'\n",
    "            ,'مثال'\n",
    "            ,'توسط'\n",
    "            ,'زیر'\n",
    "            ,'اول'\n",
    "            ,'دوم'\n",
    "            ,'سوم'\n",
    "            ,'1'\n",
    "            ,'2'\n",
    "            ,'3'\n",
    "            ,'4'\n",
    "            ,'5'\n",
    "            ,'5'\n",
    "            ,'6'\n",
    "            ,'7'\n",
    "            ,'8'\n",
    "            ,'9'\n",
    "            ,'0'\n",
    "            ,'احمد'\n",
    "            ,'+'\n",
    "            ,'بهنام'\n",
    "            ,'مریم'\n",
    "            ,'نام'\n",
    "            ,'حسن'\n",
    "            ,'مورد'\n",
    "            ,'رو'\n",
    "            ,'ترین'\n",
    "            ,'شهناز'\n",
    "            ,'حسینی'\n",
    "            ,'لیلا'\n",
    "           ]\n",
    "a = [1,2,3,5]\n",
    "for i in a:\n",
    "    i = 0\n",
    "a    \n",
    "punctuationmark = ['است','شود','شد','کرد',',','.','خواه','*','!','@','#','$','%','^','&','*','(',')','_','+','|','{',]\n",
    "len(damnword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsbodytitle = []\n",
    "newsbodydict = dict()\n",
    "for add in address:\n",
    "    df = pd.read_csv(add + 'newsbodytitle.csv')\n",
    "    newsbodytitle.append(df)\n",
    "    newsbodydict[add] = df.ix[:,0]\n",
    "print len(newsbodydict)\n",
    "print len(newsbodytitle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "allnewssentence = dict()\n",
    "for key in newsbodydict:\n",
    "    sentences = []\n",
    "    for bod in newsbodydict[key]:\n",
    "        bod = normalizer.normalize(unicode(bod))\n",
    "        sentence = sent_tokenize(bod)\n",
    "        sentences.append(sentence)\n",
    "    print len(sentences)    \n",
    "    a = []\n",
    "    normsen = []\n",
    "    for par in sentences:\n",
    "        for sen in par:\n",
    "            a.append(list(word_tokenize(sen)))\n",
    "        normsen.append(a)\n",
    "        a = []\n",
    "    print len    \n",
    "    df = pd.DataFrame(normsen)\n",
    "    df.to_csv(os.path.join(key,'normsentences.csv'), index=False, encoding='utf-8')\n",
    "    allnewssentence[key] = normsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('sensen.txt','wb')\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "eachnewsword = []\n",
    "helplist = []\n",
    "useless = []\n",
    "allnewsword = dict()\n",
    "for key in allnewssentence:\n",
    "    for sen in allnewssentence[key]:\n",
    "        for token in sen:\n",
    "            for w in token:\n",
    "                damn = False\n",
    "                for dw in damnword:\n",
    "                    if dw in w:\n",
    "                        damn = True\n",
    "                if 'های' in w[-3:]:\n",
    "                    w = stemmer.stem(w)\n",
    "                if 'ها' in w[-2:]:\n",
    "                    w = stemmer.stem(w)\n",
    "                if '-' in w:\n",
    "                    w = w.replace('-', ' ') \n",
    "                if '_' in w:\n",
    "                    w = w.replace('_',' ')\n",
    "                w = lemmatizer.lemmatize(w)    \n",
    "                if  damn or unicode.isnumeric(w) or len(w) <= 3 or ('*' in w) or ('/' in w) or ('#' in w) or ('@' in w) or  (len(w) <= 2) or  ('خواه' in w) or ('کرد' in w)  or ('است' in w) or ('ایرنا' in w) or ('گزارش' in w) or ('شد' in w) or ('شود' in w):\n",
    "                    useless.append(w)\n",
    "                else:\n",
    "                    helplist.append(w)\n",
    "            s = ''        \n",
    "            for vocab in helplist:\n",
    "                s = s + vocab + ' '\n",
    "            f.write(s + '\\n')\n",
    "            helplist=[]\n",
    "len(allword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24301"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(allword)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sens = []\n",
    "for add in address:\n",
    "    normsen = pd.read_csv(add + 'normsentences.csv')\n",
    "    normsen.fillna(0,inplace=True)\n",
    "    for row in normsen.iterrows():\n",
    "        index, data = row\n",
    "        data = list(set(data.tolist()))\n",
    "        if 0 in data:\n",
    "            data.remove(0)\n",
    "        sens = sens + data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "sens = []\n",
    "for row in normsen[key].iterrows():\n",
    "    index , data = row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'useless'\n",
    "for w in useless:\n",
    "    print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\xd8\\xb3\\xd8\\xb1\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1, \\xd9\\x82\\xd9\\x86\\xd8\\xa8\\xd8\\xb1\\xdb\\x8c, \\xd8\\xa7\\xd9\\x81\\xd8\\xb2\\xd9\\x88\\xd8\\xaf, :, \\xdb\\xb9\\xdb\\xb0, \\xd8\\xaf\\xd8\\xb1\\xd8\\xb5\\xd8\\xaf, \\xd8\\xac\\xd8\\xb1\\xd8\\xa7\\xd8\\xa6\\xd9\\x85, \\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xdb\\x8c\\xdb\\x8c, \\xd8\\xaf\\xd8\\xb1, \\xd8\\xb3\\xdb\\x8c\\xd8\\xb3\\xd8\\xaa\\xd8\\xa7\\xd9\\x86, \\xd9\\x88, \\xd8\\xa8\\xd9\\x84\\xd9\\x88\\xda\\x86\\xd8\\xb3\\xd8\\xaa\\xd8\\xa7\\xd9\\x86, \\xda\\xa9\\xd8\\xb4\\xd9\\x81, \\xd8\\xb4\\xd8\\xaf\\xd9\\x87_\\xd8\\xa7\\xd8\\xb3\\xd8\\xaa, .]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in allnewssentence:\n",
    "    for sen in allnewssentence[key]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
